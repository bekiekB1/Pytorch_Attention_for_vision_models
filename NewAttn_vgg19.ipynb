{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NewAttn_vgg19.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tt-qf-MQ0MIk",
        "outputId": "734c5398-c144-410e-d59c-4ea318496642"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wed Sep 22 02:36:34 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 470.63.01    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   72C    P8    81W / 149W |      0MiB / 11441MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LtP3s8j_0Z1f"
      },
      "source": [
        "#!pip install pretrainedmodels\n",
        "#import pretrainedmodels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fExhD0WB0gSw"
      },
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from torchvision import transforms,models\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "from skimage import io\n",
        "\n",
        "import os\n",
        "\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import copy\n",
        "import shutil\n",
        "import logging\n",
        "import torchvision.utils as utils\n",
        "\n",
        "\n",
        "\n",
        "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, roc_curve, roc_auc_score\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "cuda = torch.cuda.is_available()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fg5a1r1T0h-G",
        "outputId": "70d1b6c2-bae3-43cf-8403-859ba4f6183a"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CeLDFxcq0lmJ"
      },
      "source": [
        "def get_loader(data_path,pin_memory=True,num_workers = 8,batch_size = 256):\n",
        "    \"\"\"\n",
        "    Return the dataloader for train,dev and test, along with number of Classes in dataset\n",
        "    Args:\n",
        "        pin_memory: (bool) speed up host to device transfer(load samples on CPU push to GPU on training)\n",
        "        number_workers: (int) multi-process data loading\n",
        "        batch_size: (int) load data in batches\n",
        "    \n",
        "    Returns:\n",
        "        dataloaders: (DataLoader) train, test, and dev dataloaders\n",
        "        num_classses: (int) number of different classes of faces in dataset\n",
        "    \"\"\"\n",
        "    transform = transforms.Compose([transforms.Resize(256),\n",
        "                                    transforms.CenterCrop(224),\n",
        "                                    #transforms.Resize(32),\n",
        "                                    transforms.CenterCrop(224),\n",
        "                                    transforms.RandomHorizontalFlip(p=0.5),\n",
        "                                    transforms.RandomCrop(size=(224,224)),\n",
        "                                    transforms.GaussianBlur(kernel_size=(5, 9), sigma=(0.1, 5)),\n",
        "                                    transforms.ToTensor(),##Add more data Augumentation(Select data augumentation -> None or other options like horizontal flip crop)\n",
        "                                    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),])\n",
        "    # Train Dataset\n",
        "    train_dataset = torchvision.datasets.ImageFolder(root=data_path+'train/', \n",
        "                                                 transform = transform)\n",
        "    \n",
        "    # Dev Dataset                                             \n",
        "    dev_dataset = torchvision.datasets.ImageFolder(root=data_path+'val/', \n",
        "                                               transform = transform)\n",
        "    # Test Dataset\n",
        "    test_dataset = torchvision.datasets.ImageFolder(root=data_path+'test/', \n",
        "                                               transform = transform)\n",
        "    \n",
        "    #Trainloader\n",
        "    train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, \n",
        "                                               shuffle=True, num_workers=num_workers,pin_memory=True)\n",
        "\n",
        "    \n",
        "    # Dev Loader\n",
        "    dev_dataloader = torch.utils.data.DataLoader(dev_dataset, batch_size=batch_size, \n",
        "                                             shuffle=True, num_workers=num_workers, pin_memory=True)\n",
        "\n",
        "    # Test Loader\n",
        "    test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=10, \n",
        "                                             shuffle=False, num_workers=num_workers, pin_memory=True)\n",
        "    \n",
        "    return train_dataloader, dev_dataloader,test_dataloader, len(train_dataset.classes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vj0L8WsFy-Q9"
      },
      "source": [
        "class MyVgg(nn.Module):\n",
        "  '''\n",
        "  PreTrained VGG model(fixed Feature Extractor) with\n",
        "  Attention Mechanism\n",
        "  https://arxiv.org/pdf/1804.02391v2.pdf\n",
        "  '''\n",
        "  def __init__(self):\n",
        "    super(MyVgg,self).__init__()\n",
        "    vgg = models.vgg19_bn(pretrained=True)\n",
        "    vgg_lay = []\n",
        "    for x in vgg.children():\n",
        "      vgg_lay.append(x)\n",
        "    self.conv_feat = nn.Sequential(*vgg_lay[0],vgg_lay[1])\n",
        "    self.attention_model = nn.Sequential(nn.Conv2d(512,512,kernel_size=3,padding=1),nn.Conv2d(512,512,kernel_size=1))\n",
        "    self.fc_1 = nn.Linear(512*7*7,4096)\n",
        "    self.fc_final = nn.Linear(4608,2)\n",
        "  \n",
        "  def forward(self,x):\n",
        "    f_mn = self.conv_feat(x)\n",
        "    w_mn = self.attention_model(f_mn)\n",
        "    b,c,w,h = w_mn.shape\n",
        "    m_mn = F.softmax(w_mn.view(b,c,-1),dim=2).view(b,c,w,h)\n",
        "    f_att_all = f_mn + m_mn * f_mn\n",
        "    x = torch.flatten(f_att_all, 1)\n",
        "    x = self.fc_1(x)\n",
        "    batch,chan,_,_ = f_att_all.shape\n",
        "    x1 = F.adaptive_avg_pool2d(f_att_all,output_size=1).view(batch,chan)\n",
        "    x = torch.cat([x,x1],dim=1)\n",
        "    x = self.fc_final(x)\n",
        "    return [x,m_mn]\n",
        "\n",
        "  def freeze(self):   \n",
        "    for p in self.conv_feat.parameters():\n",
        "      p.requires_grad = False\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1MGvzgMw0xCM"
      },
      "source": [
        "class RunningAverage():\n",
        "    \"\"\"A simple class that maintains the running average of a quantity\n",
        "    \n",
        "    Example:\n",
        "    ```\n",
        "    loss_avg = RunningAverage()\n",
        "    loss_avg.update(2)\n",
        "    loss_avg.update(4)\n",
        "    loss_avg() = 3\n",
        "    ```\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        self.steps = 0\n",
        "        self.total = 0\n",
        "    \n",
        "    def update(self, val):\n",
        "        self.total += val\n",
        "        self.steps += 1\n",
        "    \n",
        "    def __call__(self):\n",
        "        return self.total/float(self.steps)\n",
        "\n",
        "\n",
        "def save_checkpoint(state, is_best, checkpoint):\n",
        "    \"\"\"Saves model and training parameters at checkpoint + 'last.pth.tar'. If is_best==True, also saves\n",
        "    checkpoint + 'best.pth.tar'\n",
        "    Args:\n",
        "        state: (dict) contains model's state_dict, may contain other keys such as epoch, optimizer state_dict\n",
        "        is_best: (bool) True if it is the best model seen till now\n",
        "        checkpoint: (string) folder where parameters are to be saved\n",
        "    \"\"\"\n",
        "    filepath = os.path.join(checkpoint, 'last.pth.tar')\n",
        "    if not os.path.exists(checkpoint):\n",
        "        print(\"Checkpoint Directory does not exist! Making directory {}\".format(checkpoint))\n",
        "        os.mkdir(checkpoint)\n",
        "    else:\n",
        "        print(\"Checkpoint Directory exists! \")\n",
        "    torch.save(state, filepath)\n",
        "    if is_best:\n",
        "        shutil.copyfile(filepath, os.path.join(checkpoint, 'best.pth.tar'))\n",
        "\n",
        "def load_checkpoint(checkpoint, model, optimizer=None,scheduler=None):\n",
        "    \"\"\"Loads model parameters (state_dict) from file_path. If optimizer is provided, loads state_dict of\n",
        "    optimizer assuming it is present in checkpoint.\n",
        "    Args:\n",
        "        checkpoint: (string) filename which needs to be loaded\n",
        "        model: (torch.nn.Module) model for which the parameters are loaded\n",
        "        optimizer: (torch.optim) optional: resume optimizer from checkpoint\n",
        "    \"\"\"\n",
        "    if not os.path.exists(checkpoint):\n",
        "        raise(\"File doesn't exist {}\".format(checkpoint))\n",
        "    #checkpoint = torch.load(checkpoint)\n",
        "    checkpoint = torch.load(checkpoint,map_location=device)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "    if optimizer:\n",
        "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "        #optimizer2.load_state_dict(checkpoint['optimizer_closs_state_dict'])\n",
        "    \n",
        "    if scheduler:\n",
        "      scheduler.load_state_dict(checkpoint[\"scheduler_save\"])\n",
        "\n",
        "    return checkpoint\n",
        "\n",
        "def set_logger(log_path):\n",
        "    \"\"\"Set the logger to log info in terminal and file `log_path`.\n",
        "    In general, it is useful to have a logger so that every output to the terminal is saved\n",
        "    in a permanent file. Here we save it to `model_dir/train.log`.\n",
        "    Example:\n",
        "    ```\n",
        "    logging.info(\"Starting training...\")\n",
        "    ```\n",
        "    Args:\n",
        "        log_path: (string) where to log\n",
        "    \"\"\"\n",
        "    logger = logging.getLogger()\n",
        "    logger.setLevel(logging.INFO)\n",
        "\n",
        "    if not logger.handlers:\n",
        "        # Logging to a file\n",
        "        file_handler = logging.FileHandler(log_path)\n",
        "        file_handler.setFormatter(logging.Formatter('%(asctime)s:%(levelname)s: %(message)s'))\n",
        "        logger.addHandler(file_handler)\n",
        "\n",
        "        # Logging to console\n",
        "        stream_handler = logging.StreamHandler()\n",
        "        stream_handler.setFormatter(logging.Formatter('%(message)s'))\n",
        "        logger.addHandler(stream_handler)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xcUcKJ7L0yoO"
      },
      "source": [
        "def cross_entropy(output, labels):\n",
        "    \"\"\"\n",
        "    Loss for Classification\n",
        "    Args:\n",
        "        output: (Tensor) Tensor of Prediction made by the model for face class\n",
        "        labels: (Tensor) True labels for each batch of faces\n",
        "    \"\"\"\n",
        "    return F.cross_entropy(output, labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oALfL8GJ01RI"
      },
      "source": [
        "def one_epoch(epoch,net,loader,optimizer,images_disp):\n",
        "  net.train()\n",
        "  running_loss = 0.0\n",
        "  n = 0\n",
        "  correct = 0\n",
        "  total = 0\n",
        "  loss_avg = RunningAverage()\n",
        "  with tqdm(total=len(loader)) as t:\n",
        "    for i,(inputs,labels) in enumerate(loader):\n",
        "      \n",
        "      inputs = inputs.to(device)\n",
        "      labels = labels.to(device)\n",
        "      if i == 0:\n",
        "        images_disp.append(inputs[0:36,:,:,:])\n",
        "      optimizer.zero_grad()\n",
        "      outputs,_ = net(inputs)\n",
        "      #outputs = net(inputs)\n",
        "      loss = cross_entropy(outputs,labels)\n",
        "      running_loss += loss.item()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      _, predicted = torch.max(outputs.data, 1)\n",
        "      total += labels.size(0)\n",
        "      correct += (predicted == labels).sum().item()\n",
        "      torch.cuda.empty_cache()\n",
        "      loss_avg.update(loss.item())\n",
        "\n",
        "      del inputs\n",
        "      del labels\n",
        "      del loss\n",
        "      t.set_postfix(loss='{:05.3f}'.format(loss_avg()))\n",
        "      t.update()\n",
        "  avg_loss = running_loss / total\n",
        "  acc = correct / total *100\n",
        "  return avg_loss,acc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vMIxp75c02lg"
      },
      "source": [
        "def infer_classfication(net, loader,images_disp):\n",
        "    net.eval()\n",
        "    running_loss = 0.0\n",
        "    n = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for i, (inputs,labels) in enumerate(loader):\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "            if i == 0:\n",
        "              images_disp.append(inputs[0:36,:,:,:])\n",
        "            outputs,_ = net(inputs)\n",
        "            #outputs = net(inputs)\n",
        "            loss = cross_entropy(outputs,labels)\n",
        "            running_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "            del inputs\n",
        "            del labels\n",
        "\n",
        "    acc = correct / total * 100\n",
        "    avg_loss = running_loss / total\n",
        "    return avg_loss, acc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mfvb_bMU04Bi"
      },
      "source": [
        "def train_step(net,loader,dev_loader,optimizer,scheduler,epochs):\n",
        "  net.train()\n",
        "  train_losses = []\n",
        "  valid_losses = []\n",
        "  valid_acc = []\n",
        "  auc_acc = []\n",
        "\n",
        "  best_acc = 0.0\n",
        "  is_best = False\n",
        "  for epoch in range(epochs):\n",
        "    images_disp = []\n",
        "    for prarm_group in optimizer.param_groups:\n",
        "      print(\"Current lr: \\t{}\".format(prarm_group[\"lr\"]))\n",
        "      writer.add_scalar('train/learning_rate', optimizer.param_groups[0]['lr'], epoch)\n",
        "  \n",
        "    avg_loss_t, acc_t = one_epoch(epoch, net, loader, optimizer,images_disp)\n",
        "\n",
        "    print(f'\\n\\n*************\\n')\n",
        "    print('Epoch [%d], loss: %.8f, acc: %.4f' %\n",
        "                (epoch + 1, avg_loss_t, acc_t))\n",
        "    avg_loss_v, acc_v = infer_classfication(net, dev_loader,images_disp)\n",
        "    writer.add_scalar('train/accuracy', acc_t, epoch)\n",
        "    writer.add_scalar('test/accuracy', acc_v, epoch)\n",
        "    writer.add_scalar('train/loss', avg_loss_t, epoch)\n",
        "    writer.add_scalar('test/loss', avg_loss_v, epoch)\n",
        "    I_train = utils.make_grid(images_disp[0], nrow=6, normalize=True, scale_each=True)\n",
        "    writer.add_image('train/image', I_train, epoch)\n",
        "    if epoch == 0:\n",
        "      I_test = utils.make_grid(images_disp[1], nrow=6, normalize=True, scale_each=True)\n",
        "      writer.add_image('test/image', I_test, epoch)\n",
        "    '''\n",
        "    min_up_factor = 8\n",
        "    vis_fun = visualize_attn_softmax\n",
        "    #vis_fun = visualize_attn_sigmoid\n",
        "    __, c1, c2, c3 = net(images_disp[0])\n",
        "    \n",
        "    if c1 is not None:\n",
        "        attn1 = vis_fun(I_train, c1, up_factor=min_up_factor, nrow=6)\n",
        "        writer.add_image('train/attention_map_1', attn1, epoch)\n",
        "    if c2 is not None:\n",
        "        attn2 = vis_fun(I_train, c2, up_factor=min_up_factor*2, nrow=6)\n",
        "        writer.add_image('train/attention_map_2', attn2, epoch)\n",
        "    if c3 is not None:\n",
        "        attn3 = vis_fun(I_train, c3, up_factor=min_up_factor*4, nrow=6)\n",
        "        writer.add_image('train/attention_map_3', attn3, epoch)\n",
        "    # test data\n",
        "    \n",
        "    __, c1, c2, c3 = net(images_disp[1])\n",
        "    if c1 is not None:\n",
        "        attn1 = vis_fun(I_test, c1, up_factor=min_up_factor, nrow=6)\n",
        "        writer.add_image('test/attention_map_1', attn1, epoch)\n",
        "    if c2 is not None:\n",
        "        attn2 = vis_fun(I_test, c2, up_factor=min_up_factor*2, nrow=6)\n",
        "        writer.add_image('test/attention_map_2', attn2, epoch)\n",
        "    if c3 is not None:\n",
        "        attn3 = vis_fun(I_test, c3, up_factor=min_up_factor*4, nrow=6)\n",
        "        writer.add_image('test/attention_map_3', attn3, epoch)\n",
        "    '''\n",
        "    valid_losses.append(avg_loss_v)\n",
        "    valid_acc.append(acc_v)   \n",
        "    print('[Classification valid] loss: %.8f, acc: %.4f\\n\\n' % (avg_loss_v, acc_v))\n",
        "    if acc_v > best_acc:\n",
        "      is_best = True\n",
        "    #scheduler.step() # StepLR\n",
        "    scheduler.step(avg_loss_v) # ReduceonPlateau\n",
        "    train_losses.append(avg_loss_t)\n",
        "    print('\\n','='*20)\n",
        "    #print(\"*** Saving Checkpoint ***\\n\")\n",
        "    \n",
        "    \n",
        "    save_checkpoint({'epoch': epoch + 1,\n",
        "                               'model_state_dict': net.state_dict(),\n",
        "                               'optimizer_state_dict': optimizer.state_dict(),\n",
        "                                'scheduler_save' : scheduler.state_dict(),\n",
        "                               'train_loss': train_losses,\n",
        "                                'dev_loss':valid_losses,\n",
        "                                'dev_acc': valid_acc,},\n",
        "                              is_best=is_best,\n",
        "                              checkpoint = hyper_param[\"ckpnt_training\"]\n",
        "                              #checkpoint=\"/content/gdrive/MyDrive/capstone/workings/Project_LocalMachine/checkpoint\"\n",
        "                              )\n",
        "    \n",
        "    \n",
        "  return train_losses, valid_losses,valid_acc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C8Zt7yNE08VU"
      },
      "source": [
        "def main():\n",
        "  # Set the logger\n",
        "  #set_logger(os.path.join('/content/gdrive/MyDrive/capstone/workings/Project_LocalMachine/checkpoint', 'train.log'))\n",
        "  # from torch.utils.tensorboard import SummaryWriter\n",
        "  # writer = SummaryWriter()\n",
        "\n",
        "  logging.info(\"Loading the datasets...\")\n",
        "  # fetch dataloaders\n",
        "  train_dataloader, dev_dataloader,test_dataloader, num_classes = get_loader(hyper_param[\"data_path\"]\n",
        "                                                                             ,num_workers=hyper_param[\"num_workers\"]\n",
        "                                                                             ,batch_size = hyper_param[\"batch_size\"])\n",
        "  logging.info(\"- done.\")\n",
        "  \n",
        "  logging.info(\"Loading the Pretrained Resnet18 Model for Transfer Learning\")\n",
        "  #Custom\n",
        "  tl_resnet50 = MyVgg()\n",
        "  tl_resnet50.freeze()\n",
        "  tl_resnet50.to(device)\n",
        "  print(tl_resnet50)\n",
        "  \n",
        "\n",
        "\n",
        "  #Model Org\n",
        "  '''\n",
        "  tl_resnet50 = torchvision.models.vgg19_bn(pretrained=True)\n",
        "  for param in tl_resnet50.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "  num_ftrs = tl_resnet50.classifier[6].in_features\n",
        "  tl_resnet50.classifier[6] = nn.Linear(num_ftrs, 2)\n",
        "  print(tl_resnet50)\n",
        "  tl_resnet50 = tl_resnet50.to(device)\n",
        "  '''\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  #Adam_optimizer = torch.optim.Adam(tl_resnet50.parameters(),lr = hyper_param[\"lr\"])\n",
        "  Adam_optimizer = optim.SGD(tl_resnet50.parameters(), lr=hyper_param[\"lr\"], momentum=0.9, weight_decay=5e-4)\n",
        "  scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(Adam_optimizer,mode='min',patience=3)\n",
        "  #scheduler = None\n",
        "  \n",
        "  '''\n",
        "  checkpoint_path =hyper_param['ckpnt_training']+'/last.pth.tar'\n",
        "  #checkpoint_path = \"/content/gdrive/MyDrive/capstone/workings/Project_LocalMachine/checkpoint/last.pth.tar\"\n",
        "  loaded_checkpoint = load_checkpoint(checkpoint_path,tl_resnet50,Adam_optimizer,scheduler)\n",
        "  train_loss,dev_loss,d_acc = loaded_checkpoint[\"train_loss\"],loaded_checkpoint[\"dev_loss\"], loaded_checkpoint[\"dev_acc\"]\n",
        "  print('='*20)\n",
        "  print(train_loss,dev_loss,d_acc)\n",
        "  print('='*20)\n",
        "  '''\n",
        "  \n",
        "  logging.info(f\"Starting training for {hyper_param['numEpochs']} epoch(s)\")\n",
        "  train_losses, valid_losses, valid_acc = train_step(tl_resnet50, train_dataloader, dev_dataloader,Adam_optimizer,scheduler,hyper_param[\"numEpochs\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vyi0vIsd05S1"
      },
      "source": [
        "from torch.utils.tensorboard import SummaryWriter\n",
        "writer = SummaryWriter()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EHSPUmPE1BI9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e61dd58f-efe3-4ac4-dd31-06154809accd"
      },
      "source": [
        "if __name__ == \"__main__\":\n",
        "  hyper_param = {\n",
        "    \"lr\": 1e-3,\n",
        "    \"batch_size\": 64,\n",
        "    \"numEpochs\": 100, \n",
        "    \"num_workers\": 2,\n",
        "    \"ckpnt_training\": \"/content/gdrive/MyDrive/capstone/new_checkpoint\",\n",
        "    #\"ckpnt_training\": \"/content/gdrive/MyDrive/capstone/checkpoint/newdata_checkpoint\",\n",
        "    \"checkpoint\":\"/content/gdrive/MyDrive/capstone/new_checkpoint/last.pth.tar\",\n",
        "    \"data_path\":\"/content/gdrive/MyDrive/capstone/comprs_images_boom/\"\n",
        "    #'data_path':'/content/gdrive/MyDrive/capstone/kag_data/data/'\n",
        "  }\n",
        "  main()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MyVgg(\n",
            "  (conv_feat): Sequential(\n",
            "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU(inplace=True)\n",
            "    (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (5): ReLU(inplace=True)\n",
            "    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (7): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (9): ReLU(inplace=True)\n",
            "    (10): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (11): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (12): ReLU(inplace=True)\n",
            "    (13): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (14): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (15): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (16): ReLU(inplace=True)\n",
            "    (17): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (18): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (19): ReLU(inplace=True)\n",
            "    (20): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (21): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (22): ReLU(inplace=True)\n",
            "    (23): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (24): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (25): ReLU(inplace=True)\n",
            "    (26): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (27): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (28): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (29): ReLU(inplace=True)\n",
            "    (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (31): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (32): ReLU(inplace=True)\n",
            "    (33): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (34): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (35): ReLU(inplace=True)\n",
            "    (36): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (37): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (38): ReLU(inplace=True)\n",
            "    (39): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (40): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (41): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (42): ReLU(inplace=True)\n",
            "    (43): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (44): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (45): ReLU(inplace=True)\n",
            "    (46): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (47): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (48): ReLU(inplace=True)\n",
            "    (49): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (50): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (51): ReLU(inplace=True)\n",
            "    (52): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (53): AdaptiveAvgPool2d(output_size=(7, 7))\n",
            "  )\n",
            "  (attention_model): Sequential(\n",
            "    (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
            "  )\n",
            "  (fc_1): Linear(in_features=25088, out_features=4096, bias=True)\n",
            "  (fc_final): Linear(in_features=4608, out_features=2, bias=True)\n",
            ")\n",
            "Current lr: \t0.001\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 47/47 [00:48<00:00,  1.03s/it, loss=0.629]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "*************\n",
            "\n",
            "Epoch [1], loss: 0.01002757, acc: 67.3898\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Classification valid] loss: 0.00897197, acc: 77.1739\n",
            "\n",
            "\n",
            "\n",
            " ====================\n",
            "Checkpoint Directory exists! \n",
            "Current lr: \t0.001\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 47/47 [00:53<00:00,  1.14s/it, loss=0.571]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "*************\n",
            "\n",
            "Epoch [2], loss: 0.00909973, acc: 72.4746\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Classification valid] loss: 0.00841669, acc: 78.5326\n",
            "\n",
            "\n",
            "\n",
            " ====================\n",
            "Checkpoint Directory exists! \n",
            "Current lr: \t0.001\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 47/47 [00:54<00:00,  1.17s/it, loss=0.541]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "*************\n",
            "\n",
            "Epoch [3], loss: 0.00862523, acc: 74.7797\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Classification valid] loss: 0.00816805, acc: 77.7174\n",
            "\n",
            "\n",
            "\n",
            " ====================\n",
            "Checkpoint Directory exists! \n",
            "Current lr: \t0.001\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 47/47 [00:53<00:00,  1.14s/it, loss=0.528]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "*************\n",
            "\n",
            "Epoch [4], loss: 0.00841055, acc: 76.3729\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Classification valid] loss: 0.00831109, acc: 77.7174\n",
            "\n",
            "\n",
            "\n",
            " ====================\n",
            "Checkpoint Directory exists! \n",
            "Current lr: \t0.001\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 47/47 [00:53<00:00,  1.14s/it, loss=0.505]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "*************\n",
            "\n",
            "Epoch [5], loss: 0.00805112, acc: 76.6102\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Classification valid] loss: 0.00823471, acc: 78.8043\n",
            "\n",
            "\n",
            "\n",
            " ====================\n",
            "Checkpoint Directory exists! \n",
            "Current lr: \t0.001\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 47/47 [00:54<00:00,  1.16s/it, loss=0.506]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "*************\n",
            "\n",
            "Epoch [6], loss: 0.00806773, acc: 75.9661\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Classification valid] loss: 0.00835148, acc: 76.9022\n",
            "\n",
            "\n",
            "\n",
            " ====================\n",
            "Checkpoint Directory exists! \n",
            "Current lr: \t0.001\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 47/47 [00:56<00:00,  1.21s/it, loss=0.475]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "*************\n",
            "\n",
            "Epoch [7], loss: 0.00756738, acc: 77.7627\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Classification valid] loss: 0.00829376, acc: 76.9022\n",
            "\n",
            "\n",
            "\n",
            " ====================\n",
            "Checkpoint Directory exists! \n",
            "Current lr: \t0.0001\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 47/47 [00:59<00:00,  1.26s/it, loss=0.471]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "*************\n",
            "\n",
            "Epoch [8], loss: 0.00749993, acc: 79.4576\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Classification valid] loss: 0.00827265, acc: 76.3587\n",
            "\n",
            "\n",
            "\n",
            " ====================\n",
            "Checkpoint Directory exists! \n",
            "Current lr: \t0.0001\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 47/47 [00:56<00:00,  1.20s/it, loss=0.457]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "*************\n",
            "\n",
            "Epoch [9], loss: 0.00727825, acc: 79.8644\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Classification valid] loss: 0.00811094, acc: 77.9891\n",
            "\n",
            "\n",
            "\n",
            " ====================\n",
            "Checkpoint Directory exists! \n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-66-2f97e1d71ba3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m#'data_path':'/content/gdrive/MyDrive/capstone/kag_data/data/'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m   }\n\u001b[0;32m---> 13\u001b[0;31m   \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-64-1111727778e0>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m   \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Starting training for {hyper_param['numEpochs']} epoch(s)\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m   \u001b[0mtrain_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtl_resnet50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev_dataloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mAdam_optimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhyper_param\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"numEpochs\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-63-c931501fa3fb>\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(net, loader, dev_loader, optimizer, scheduler, epochs)\u001b[0m\n\u001b[1;32m     77\u001b[0m                                 'dev_acc': valid_acc,},\n\u001b[1;32m     78\u001b[0m                               \u001b[0mis_best\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_best\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m                               \u001b[0mcheckpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhyper_param\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"ckpnt_training\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m                               \u001b[0;31m#checkpoint=\"/content/gdrive/MyDrive/capstone/workings/Project_LocalMachine/checkpoint\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m                               )\n",
            "\u001b[0;32m<ipython-input-59-0cc540a0961d>\u001b[0m in \u001b[0;36msave_checkpoint\u001b[0;34m(state, is_best, checkpoint)\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mis_best\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0mshutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopyfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'best.pth.tar'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mscheduler\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/shutil.py\u001b[0m in \u001b[0;36mcopyfile\u001b[0;34m(src, dst, follow_symlinks)\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfsrc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfdst\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m                 \u001b[0mcopyfileobj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfsrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfdst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdst\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/shutil.py\u001b[0m in \u001b[0;36mcopyfileobj\u001b[0;34m(fsrc, fdst, length)\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0;34m\"\"\"copy data from file-like object fsrc to file-like object fdst\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m         \u001b[0mbuf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfsrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlength\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0wvBY7egevPU"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}